# AWS Cloud Data Engineer Compass Uol

## Apresentação

- Sou Heliton, estou cursando o 5º semestre da graduação tecnológica de **Big Data e Inteligência Analítica** da ***Uniasselvi***.
- Moro em Uberaba, que é localizado no Triângulo Mineiro - Minas Gerais.
- Na área de tecnologia, tenho estudado Python, Linux, Docker, MySql, PostgreSql, Git, Databricks, Hadoop e alguns serviços da AWS:
    - RDS
    - DMS
    - S3
    - EMR
    - Glue
    - Athena

- Hobbies:
    - Ouvir música;
    - Ler;
    - Sair com amigos/familiares;
    - Estudar.

## Sprint 1

### Git e GitHub Básico ao Avançado

- Introdução e instalação das dependências:
    - Foi feito vídeo introdutório sobre o Git;
    - Instalação do Git em vários SOs;
    - Instalação do VS Code;
    - Explicação sobre Controle de Versão.

- Git fundamental:
    - Introdução sobre repositórios;
    - Comandos básicos do Git.

- Trabalhando com branches:
    - O que são branches;
    - Criação, delete, mundança, união de branches;
    - Stash;
    - Tags.

- Compartilhamento e atualização de repositórios;
- Análise e inspeção de repositórios;
- Administração de repositórios;
- Melhorando os commits do projeto;
- Explorando e entendendo o GitHub;
- Markdown do básico ao avançado.

### Administração de Sistemas GNU/Linux: Fundamentos e Prática

- Introdução ao Shell;
- Comandos básicos linux;
- Manipulação de dados;
- Gerenciamento do hardware;
- Editores de texto;
- Administração de usuários e grupos;
- Gerenciamento de processos;
- Shell script;
- Gerenciamento de pacotes.

## Sprint 2

### SQL para Análise de Dados: Do básico ao avançado

- Concluído curso de SQL com foco em análise;
- Usando queries para fazer consultas em SGBDs para times de negócios.

### Data & Analytics - PB - AWS

- Exercicíos e atividades propostas relacionados ao curso SQL para Análise de Dados: Do básico ao avançado;
- Reuniões diárias para averiguar as etapas e acompanhamento das atividades;
- Apresentação e explicação das atividades propostas.

## Sprint 3

### Curso Python 3

- Concluído curso de Python com foco em:
    - Fundamentos;
    - Estruturas de Controle;
    - Manipulação de arquivos;
    - Funções;
    - POO.
    
## Sprint 4

- Concluído cursos de: 

- Python com foco em:
    - Programação Funcional;
- Docker;
- Estatística Descritiva com Python
- Exercícios e desafios

## Sprint 5

- Concluído cursos de: 

- AWS Partner: Sales Accreditation (Business) (Portuguese);
- AWS Partner: Cloud Quest Practitioner;
- AWS Partner: Accreditation;
- AWS Partner: Cloud Economics Accreditation
- AWS Partner: AWS Certified Cloud Practitioner

  ## Sprint 6

- AWS Skill Builder - Data Analytics Fundamentals (Portuguese)
- AWS Partner: Data Analytics on AWS (Business) (Portuguese)
- AWS Skill Builder - Introduction to Amazon Kinesis Streams
- AWS Skill Builder - Introduction to Amazon Kinesis Analytics
- AWS Skill Builder - Introduction to Amazon Elastic MapReduce (EMR) (Portuguese)
- AWS Skill Builder - Introduction to Amazon Athena (Portuguese)
- AWS Skill Builder - Introduction to Amazon Quicksight (Portuguese)
- AWS Skill Builder - Introduction to AWS IoT Analytics
- AWS Skill Builder - Getting Started with Amazon Redshift
- AWS Skill Builder - Deep Dive into Concepts and Tools for Analyzing Streaming Data (Portuguese)
- AWS Skill Builder - Best Practices for Data Warehousing with Amazon Redshift (Portuguese)
- AWS Skill Builder - Serverless Analytics (Portuguese)
- AWS Skill Builder - Why Analytics for Games (Portuguese)
- Exercícios Propostos

## Sprint 7

Cursos:
- Learn By Example: Hadoop, MapReduce for Big Data problems
- Formação Spark com Pyspark : o Curso Completo

- Exercícios Propostos
- Desafio Parte I: Criar código Python que carrega arquivos CSV para a Nuvem utilizando as técnicas de ETL

## Sprint 8

- Exercícios Propostos
- Desafio Parte II: Fazer ingestão dos dados da API do Tmdb com Python usando lib do Boto3 para o AWS S3 da lista de filmes para a camada Raw.

## Sprint 9

- Criação de Jobs em Pyspark para fazer ingestão, transformação dos arquivos csv e json e carregamento destes em formato parquet na camada Trusted no S3 usando Aws Glue;
- Criação do Database para fazer o catálogo de dados e modelagem dimensional via Athena dos arquivos parquets criados na camada Trusted;
- Criação de Jobs em Pyspark para fazer ingestão, transformação das tabelas e views das dimensões e fatos da camada Trusted e carregamento destes em formato parquet na camada Refined no S3 usando Aws Glue;
- Criação do Database para fazer o catálogo de dados e modelagem dimensional via Athena dos arquivos parquets criados na camada Refined;














